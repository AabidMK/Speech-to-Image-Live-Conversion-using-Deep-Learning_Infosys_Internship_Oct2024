{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text: a dog in a ferest\n",
      "Corrected Text: a dog in a forest\n",
      "Enhanced Prompt for Image Generation: Generate a detailed image : a dog in a forest\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n",
    "\n",
    "audio_path = \"S2A\\Codes\\dog.mp3\"\n",
    "audio, original_rate = torchaudio.load(audio_path)\n",
    "\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=original_rate, new_freq=16000)\n",
    "audio = resampler(audio)\n",
    "\n",
    "input_features = processor(audio.squeeze(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "predicted_ids = model.generate(input_features)\n",
    "transcribed_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(f\"Transcribed Text: {transcribed_text}\")\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "input_text = f\"Correct the following text: {transcribed_text}\"\n",
    "inputs = t5_tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = t5_model.generate(**inputs, max_length=100)\n",
    "corrected_text = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Corrected Text: {corrected_text}\")\n",
    "\n",
    "enhanced_prompt = f\"Generate a detailed image of: {corrected_text}\"\n",
    "print(f\"Enhanced Prompt for Image Generation: {enhanced_prompt}\")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image = pipe(enhanced_prompt).images[0]\n",
    "image.show()\n",
    "image.save(\"output_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text: a man killing child\n",
      "Corrected Text: a  man killing child\n",
      "Warning: The generated prompt may contain sensitive content.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n",
    "\n",
    "audio_path = \"S2A\\Codes\\vaudio.mp3\"\n",
    "audio, original_rate = torchaudio.load(audio_path)\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=original_rate, new_freq=16000)\n",
    "audio = resampler(audio)\n",
    "\n",
    "input_features = processor(audio.squeeze(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "predicted_ids = model.generate(input_features)\n",
    "transcribed_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(f\"Transcribed Text: {transcribed_text}\")\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "input_text = f\"Correct the following text: {transcribed_text}\"\n",
    "inputs = t5_tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = t5_model.generate(**inputs, max_length=100)\n",
    "corrected_text = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Corrected Text: {corrected_text}\")\n",
    "\n",
    "volatile_words = [\"nude\", \"violent\",\"blood\", \"sexual\", \"pornographic\", \"erotic\", \"sensual\", \"suggestive\",\"seductive\", \"abusive\", \"vulgar\", \"immoral\", \"distasteful\",\"killing\",\"abusive\"]\n",
    "enhanced_prompt = f\"Generate a detailed image of: {corrected_text}\"\n",
    "\n",
    "if any(word in enhanced_prompt.lower() for word in volatile_words):\n",
    "    print(\"Warning: The generated prompt may contain sensitive content.\")\n",
    "\n",
    "print(f\"Enhanced Prompt for Image Generation: {enhanced_prompt}\")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image = pipe(enhanced_prompt).images[0]\n",
    "image.show()\n",
    "image.save(\"output_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'POSITIVE', 'confidence': 0.9993454818725586}\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "\n",
    "def analyze_sentiment_with_flair(text):\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    sentiment = sentence.labels[0].value\n",
    "    confidence = sentence.labels[0].score\n",
    "    \n",
    "    return {\"sentiment\": sentiment, \"confidence\": confidence}\n",
    "\n",
    "sample_text = \"A man walking in a road\"\n",
    "\n",
    "result = analyze_sentiment_with_flair(sample_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'NEGATIVE','confidence': 0.998,'violence_detected': True}\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from better_profanity import profanity\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "profanity.load_censor_words()\n",
    "\n",
    "def analyze_sentiment_with_flair_and_check_violence(text):\n",
    "    if profanity.contains_profanity(text):\n",
    "        violence_detected = True\n",
    "    else:\n",
    "        violence_detected = False\n",
    "    \n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    \n",
    "    sentiment = sentence.labels[0].value\n",
    "    confidence = sentence.labels[0].score\n",
    "    \n",
    "    result = {\n",
    "        \"sentiment\": sentiment,\n",
    "        \"confidence\": confidence,\n",
    "        \"violence_detected\": violence_detected\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "sample_text = \"A man killing a dog\"\n",
    "result = analyze_sentiment_with_flair_and_check_violence(sample_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text: A dog in a forest\n",
      "Sentiment: POSITIVE, Confidence: 0.9993454818725678, Violence Detected: False\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from better_profanity import profanity\n",
    "import io\n",
    "\n",
    "whisper_model_path = \"S2A\\Codes\\whisper_finetuned.pkl\"\n",
    "processor = WhisperProcessor.from_pretrained(whisper_model_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(whisper_model_path)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "stable_diffusion_model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(stable_diffusion_model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "profanity.load_censor_words()\n",
    "\n",
    "def record_audio(duration=10, sample_rate=16000):\n",
    "    print(\"Recording...\")\n",
    "    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    print(\"Recording finished.\")\n",
    "    return audio.flatten()\n",
    "\n",
    "def transcribe_audio(audio_data):\n",
    "    input_features = processor(audio_data.numpy(), sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    input_features = input_features.to(\"cuda\")\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    return processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "def generate_image(prompt, output_path):\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        image = pipe(prompt).images[0]\n",
    "    image.save(output_path)\n",
    "\n",
    "def analyze_sentiment_and_check_profanity(text):\n",
    "    if profanity.contains_profanity(text):\n",
    "        violence_detected = True\n",
    "    else:\n",
    "        violence_detected = False\n",
    "    \n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    \n",
    "    sentiment = sentence.labels[0].value\n",
    "    confidence = sentence.labels[0].score\n",
    "    \n",
    "    result = {\n",
    "        \"sentiment\": sentiment,\n",
    "        \"confidence\": confidence,\n",
    "        \"violence_detected\": violence_detected\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_data = record_audio(duration=10)\n",
    "    audio_data = torch.tensor(audio_data, dtype=torch.float32)\n",
    "    transcribed_text = transcribe_audio(audio_data)\n",
    "    print(f\"Transcribed Text: {transcribed_text}\")\n",
    "    \n",
    "    sentiment_result = analyze_sentiment_and_check_profanity(transcribed_text)\n",
    "    print(f\"Sentiment: {sentiment_result['sentiment']}, Confidence: {sentiment_result['confidence']}, Violence Detected: {sentiment_result['violence_detected']}\")\n",
    "    \n",
    "    generate_image(transcribed_text, \"otman_img.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
