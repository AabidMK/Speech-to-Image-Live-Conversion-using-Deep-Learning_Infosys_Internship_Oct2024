{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCNgtWf0i6bbRhjesf2BaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AabidMK/Speech-to-Image-Live-Conversion-using-Deep-Learning_Infosys_Internship_Oct2024/blob/Neeharika/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Load the fine-tuned Whisper model\n",
        "asr_model = pipeline(\"automatic-speech-recognition\", model=\"whisper_finetuned\")\n",
        "sentiment_model = pipeline(\"sentiment-analysis\")\n",
        "# Function to record audio\n",
        "def record_audio(duration=5, samplerate=16000):\n",
        "    st.write(\"Recording...\")\n",
        "    audio = sd.rec(int(samplerate * duration), samplerate=samplerate, channels=1, dtype='float32')\n",
        "    sd.wait()\n",
        "    st.write(\"Recording finished.\")\n",
        "    return audio.flatten()\n",
        "\n",
        "# Function to transcribe audio using Whisper\n",
        "def transcribe_audio(audio):\n",
        "    result = asr_model(audio)\n",
        "    return result['text']\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Audio Transcription and Image Generation\")\n",
        "\n",
        "    # Record audio\n",
        "    if st.button(\"Record Audio\"):\n",
        "        duration = st.slider(\"Select recording duration in seconds\", 1, 10, 5)\n",
        "        audio = record_audio(duration)\n",
        "        text = transcribe_audio(audio)\n",
        "        st.write(f\"Recognized: {text}\")\n",
        "        # Perform sentiment analysis\n",
        "        sentiment = sentiment_model(text)\n",
        "        st.write(f\"Sentiment: {sentiment[0]['label']} with score {sentiment[0]['score']:.2f}\")\n",
        "\n",
        "        # Load the stable diffusion model\n",
        "        model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
        "\n",
        "        # Generate the image\n",
        "        with torch.autocast(\"cuda\"):\n",
        "            image = pipe(text, guidance_scale=7.5).images[0]\n",
        "\n",
        "        # Display the image\n",
        "        st.image(image, caption=\"Generated Image\", use_column_width=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "30iNaIjZMZib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import torch\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Whisper Fine-Tuning Interface\")\n",
        "\n",
        "st.write(\"This interface allows you to fine-tune the Whisper model on a custom audio dataset.\")\n",
        "\n",
        "# File paths\n",
        "csv_path = st.text_input(\"Enter path to the CSV file with audio metadata:\", 'C:/Users/neeha/Downloads/Dataset/Dataset/Recordings/audio__details.csv')\n",
        "audio_folder_path = st.text_input(\"Enter path to the audio folder:\", 'C:/Users/neeha/Downloads/Dataset/Dataset/Recordings/Train')\n",
        "\n",
        "# Load dataset and prepare it for training\n",
        "st.write(\"Loading and preparing the dataset...\")\n",
        "df = pd.read_csv(csv_path)\n",
        "df['File_name'] = df['File_name'].apply(lambda x: os.path.abspath(os.path.join(audio_folder_path, os.path.basename(x))))\n",
        "df.to_csv(csv_path, index=False)  # Save the updated CSV\n",
        "\n",
        "dataset = load_dataset('csv', data_files=csv_path)\n",
        "dataset = dataset.cast_column('File_name', Audio(sampling_rate=16000))\n",
        "dataset = dataset.rename_column('File_name', 'audio')\n",
        "dataset = dataset.rename_column('phrase', 'sentence')\n",
        "\n",
        "# Load processor\n",
        "st.write(\"Loading Whisper Processor...\")\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "# Function to preprocess audio and text\n",
        "def prepare_dataset(batch):\n",
        "    batch[\"input_features\"] = processor(batch[\"audio\"][\"array\"], sampling_rate=16000).input_features[0]\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "# Filter columns and preprocess dataset\n",
        "columns_to_remove = [\n",
        "    'audio_clipping', 'audio_clipping:confidence', 'background_noise_audible',\n",
        "    'background_noise_audible:confidence', 'overall_quality_of_the_audio',\n",
        "    'quiet_speaker', 'quiet_speaker:confidence', 'speaker_id', 'file_download',\n",
        "    'prompt', 'writer_id'\n",
        "]\n",
        "columns_to_remove = [col for col in columns_to_remove if col in dataset.column_names]\n",
        "dataset = dataset.map(prepare_dataset, remove_columns=columns_to_remove)\n",
        "\n",
        "# Load model\n",
        "st.write(\"Loading Whisper Model...\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "# Training parameters\n",
        "st.subheader(\"Training Parameters\")\n",
        "output_dir = st.text_input(\"Output directory for fine-tuned model:\", \"whisper-finetuned\")\n",
        "batch_size = st.slider(\"Per-device train batch size:\", 1, 16, 8)\n",
        "gradient_accumulation_steps = st.slider(\"Gradient accumulation steps:\", 1, 8, 4)\n",
        "learning_rate = st.number_input(\"Learning rate:\", min_value=1e-6, max_value=1e-3, value=1e-5, step=1e-6, format=\"%.6f\")\n",
        "num_train_epochs = st.slider(\"Number of training epochs:\", 1, 10, 3)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    fp16=False,\n",
        "    save_steps=100,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Data Collator\n",
        "class DataCollatorForWhisper:\n",
        "    def __call__(self, features):\n",
        "        input_features = [torch.tensor(feature[\"input_features\"]) for feature in features]\n",
        "        labels = [torch.tensor(feature[\"labels\"]) for feature in features]\n",
        "        input_features_padded = pad_sequence(input_features, batch_first=True, padding_value=0)\n",
        "        labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "        return {\"input_features\": input_features_padded, \"labels\": labels_padded}\n",
        "\n",
        "data_collator = DataCollatorForWhisper()\n",
        "\n",
        "# Fine-tuning\n",
        "if st.button(\"Start Training\"):\n",
        "    st.write(\"Initializing Trainer...\")\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        tokenizer=processor.tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    st.write(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    st.write(\"Saving fine-tuned model...\")\n",
        "    model.save_pretrained(output_dir)\n",
        "    processor.save_pretrained(output_dir)\n",
        "\n",
        "    st.success(\"Model fine-tuning completed and saved!\")\n"
      ],
      "metadata": {
        "id": "fOEudIWFO8lc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}