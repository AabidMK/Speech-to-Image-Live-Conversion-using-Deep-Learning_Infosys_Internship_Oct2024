{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMeitJADCEKaGBDyTGGy09",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AabidMK/Speech-to-Image-Live-Conversion-using-Deep-Learning_Infosys_Internship_Oct2024/blob/Neeharika/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import streamlit as st\n",
        "import sounddevice as sd\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import librosa\n",
        "from time import sleep\n",
        "from transformers import pipeline\n",
        "\n",
        "# Paths to models\n",
        "whisper_model_path = \"C:/Users/prash/whisper-finetuned-v2\"\n",
        "sd_model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load Whisper model and processor\n",
        "processor = WhisperProcessor.from_pretrained(whisper_model_path)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(whisper_model_path).to(device)\n",
        "\n",
        "# Load Stable Diffusion model\n",
        "pipe = StableDiffusionPipeline.from_pretrained(sd_model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)  # Move Stable Diffusion model to GPU\n",
        "\n",
        "# Initialize HuggingFace BERT sentiment analysis pipeline (using GPU if available)\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# Streamlit UI setup\n",
        "st.set_page_config(page_title=\"Speech-to-Image Generator\", layout=\"wide\")\n",
        "\n",
        "# Custom CSS for a more sophisticated design\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    /* App Background and Layout */\n",
        "    .stApp {\n",
        "        background-color: #f4f7fb;\n",
        "        font-family: \"Arial\", sans-serif;\n",
        "    }\n",
        "\n",
        "    /* Title Styling */\n",
        "    .title-container {\n",
        "        text-align: center;\n",
        "        margin-top: 40px;\n",
        "        margin-bottom: 40px;\n",
        "    }\n",
        "\n",
        "    .title-container h1 {\n",
        "        font-size: 42px;\n",
        "        color: #333;\n",
        "        font-weight: 600;\n",
        "        letter-spacing: 1px;\n",
        "    }\n",
        "\n",
        "    .title-container p {\n",
        "        font-size: 18px;\n",
        "        color: #555;\n",
        "        font-weight: 400;\n",
        "    }\n",
        "\n",
        "    /* Button Styling */\n",
        "    .stButton > button {\n",
        "        background-color: #5C6BC0;\n",
        "        color: white;\n",
        "        border-radius: 50px;\n",
        "        padding: 14px 28px;\n",
        "        font-size: 18px;\n",
        "        border: none;\n",
        "        transition: background-color 0.3s, transform 0.2s ease-in-out;\n",
        "        box-shadow: 0px 8px 16px rgba(0, 0, 0, 0.15);\n",
        "    }\n",
        "\n",
        "    .stButton > button:hover {\n",
        "        background-color: #3F51B5;\n",
        "        transform: scale(1.05);\n",
        "    }\n",
        "\n",
        "    /* Slider Styling */\n",
        "    .stSlider > div {\n",
        "        margin-top: 15px;\n",
        "        width: 60%;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "\n",
        "    /* Results Card Styling */\n",
        "    .result-card {\n",
        "        background-color: #ffffff;\n",
        "        padding: 30px;\n",
        "        border-radius: 16px;\n",
        "        box-shadow: 0px 8px 24px rgba(0, 0, 0, 0.1);\n",
        "        margin-top: 20px;\n",
        "        margin-bottom: 30px;\n",
        "        width: 80%;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "\n",
        "    .result-card p {\n",
        "        font-size: 20px;\n",
        "        color: #333;\n",
        "        font-weight: 600;\n",
        "        text-align: center;\n",
        "    }\n",
        "\n",
        "    .result-card .text {\n",
        "        font-size: 18px;\n",
        "        color: #555;\n",
        "        text-align: center;\n",
        "        line-height: 1.8;\n",
        "    }\n",
        "\n",
        "    /* Sentiment Styling */\n",
        "    .sentiment-positive {\n",
        "        color: #4caf50;\n",
        "    }\n",
        "\n",
        "    .sentiment-negative {\n",
        "        color: #e53935;\n",
        "    }\n",
        "\n",
        "    /* Image Styling */\n",
        "    .stImage {\n",
        "        border-radius: 12px;\n",
        "        box-shadow: 0px 4px 16px rgba(0, 0, 0, 0.2);\n",
        "        margin-top: 20px;\n",
        "        width: 70%;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "\n",
        "    /* Footer Styling */\n",
        "    .footer {\n",
        "        font-size: 16px;\n",
        "        color: #777;\n",
        "        text-align: center;\n",
        "        padding-top: 40px;\n",
        "        padding-bottom: 40px;\n",
        "    }\n",
        "\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "# Main title and subtitle\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <div class=\"title-container\">\n",
        "        <h1>Speech-to-Image Generator</h1>\n",
        "        <p>Record your audio, transcribe it, perform sentiment analysis, and generate an image based on the transcription.</p>\n",
        "    </div>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "# Audio recording duration slider (centered and wider)\n",
        "duration = st.slider(\n",
        "    \"Select recording duration (seconds)\",\n",
        "    min_value=1,\n",
        "    max_value=30,\n",
        "    value=15,\n",
        "    step=1,\n",
        "    help=\"Drag the slider to set the duration of your audio recording.\",\n",
        ")\n",
        "\n",
        "# Record audio button\n",
        "if st.button(\"Start Recording üéôÔ∏è\"):\n",
        "    fs = 16000\n",
        "\n",
        "    st.write(\"**Recording... Please speak clearly.**\")\n",
        "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=\"float32\")\n",
        "    sd.wait()\n",
        "    st.success(\"Recording complete!\")\n",
        "\n",
        "    # Save recorded audio to a file\n",
        "    audio_path = \"audio_input.wav\"\n",
        "    write(audio_path, fs, (audio * 32767).astype(np.int16))\n",
        "\n",
        "    # Transcribe audio with Whisper\n",
        "    st.write(\"**Transcribing audio...**\")\n",
        "    audio_input, _ = librosa.load(audio_path, sr=16000)\n",
        "    input_features = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
        "    predicted_ids = model.generate(input_features)\n",
        "    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Display transcribed text with enhanced styling\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "        <div class=\"result-card\">\n",
        "            <p><strong>Transcription:</strong></p>\n",
        "            <p class=\"text\">{transcription}</p>\n",
        "        </div>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    st.write(\"üìä **Analyzing sentiment...**\")\n",
        "    sentiment = sentiment_analysis(transcription)\n",
        "    sentiment_label = sentiment[0][\"label\"]\n",
        "    sentiment_score = sentiment[0][\"score\"]\n",
        "\n",
        "    # Display sentiment analysis with enhanced styling\n",
        "    sentiment_class = \"sentiment-positive\" if sentiment_label == \"POSITIVE\" else \"sentiment-negative\"\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "        <div class=\"result-card\">\n",
        "            <p><strong>Sentiment Analysis:</strong></p>\n",
        "            <p class=\"{sentiment_class}\">{sentiment_label} (Confidence: {sentiment_score:.2f})</p>\n",
        "        </div>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    if sentiment_label == \"NEGATIVE\":\n",
        "        st.warning(\"The sentiment is negative. No image will be generated.\")\n",
        "    else:\n",
        "        st.write(\"**Generating image from text...**\")\n",
        "        with st.spinner(\"This may take a few seconds...\"):\n",
        "            sleep(2)\n",
        "            with torch.no_grad():\n",
        "                image = pipe(transcription).images[0]\n",
        "            st.image(image, caption=\"Generated Image\", use_column_width=True)\n",
        "\n",
        "# Footer section\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <div class=\"footer\">\n",
        "        **Instructions:**\n",
        "        1. Adjust the recording duration using the slider.\n",
        "        2. Click **Start Recording** to begin the audio capture.\n",
        "        3. View the transcription, sentiment analysis, and image generation output below.\n",
        "    </div>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")"
      ],
      "metadata": {
        "id": "xKDD0vQh94kc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}